{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script reads in a text and tries to identify the passages containing most smell terms\n",
    "# 10 sentences before and 10 sentences after the 'smelly passage' are also selected\n",
    "# update line 10 to choose a language \n",
    "\n",
    "# Marieke.van.Erp@dh.huc.knaw.nl\n",
    "# 7 July 2021\n",
    "\n",
    "import re \n",
    "import spacy \n",
    "nlp = spacy.load(\"nl_core_news_sm\")  # nlp = spacy.load(\"de_core_news_sm\")  for German \n",
    "import glob \n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download nl_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from: https://github.com/Odeuropa/findSmellWords/blob/main/findSmellWords-generic.py\n",
    "# update line 22 to choose a language \n",
    "def LoadRegexes(fn):\n",
    "    \"\"\"\n",
    "    This function reads the plain-text file 'fn' \n",
    "    and assumes that each line is a regular expression.\n",
    "    It returns a corresponding list of Python's regex objects.\n",
    "    \"\"\"\n",
    "    f = open(fn, \"rt\")\n",
    "    res = []\n",
    "    for s in f:\n",
    "        s = s.strip()\n",
    "        if not s: continue\n",
    "        # s = s.replace(\"@\", \"(.?)\").replace(\"*\", \".*\")\n",
    "        #print(s)\n",
    "        res.append(re.compile(s))\n",
    "    f.close()\n",
    "    #print(res)\n",
    "    print(\"%d regexes loaded from \\\"%s\\\".\" % (len(res), fn))\n",
    "    return res\n",
    "\n",
    "fnRegexes = \"dutch_smell_regexp.txt\" # fnRegexes = \"du_smell_regexp.txt\" for German \n",
    "smellWordRegexes = LoadRegexes(fnRegexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from: https://github.com/Odeuropa/findSmellWords/blob/main/findSmellWords-generic.py\n",
    "smellWordCache = {}\n",
    "def IsSmellWord(s):\n",
    "    \"\"\"\n",
    "    Tests if 's' matches any of the regular expressions in 'smellWordRegexes'.\n",
    "    \"\"\"\n",
    "    global smellWordCache\n",
    "    if s in smellWordCache: return smellWordCache[s]\n",
    "    result = False\n",
    "    for re in smellWordRegexes:\n",
    "        if re.match(s): result = True; break\n",
    "    smellWordCache[s] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustments in line 5 and line 80 for the input documents and where you want to write the tracking too. \n",
    "\n",
    "# Make sure the texts aren't too long: for x in *txt ; do split -l 1000 $x $x ; done \n",
    "\n",
    "for name in glob.glob(\"/Users/marieke/Downloads/wetransfer_teksten_2021-09-14_1942/textsplits/*\"):\n",
    "#for name in glob.glob(\"test.txt\"):\n",
    "    print(name)\n",
    "    name_elems = name.split('/')\n",
    "    outputfile = name_elems[-1] + \"_potentially_smelly_passages.txt\"\n",
    "    #outputfile = name + \"_potentially_smelly_passages.txt\"\n",
    "    # Open a text file and turn it into a spaCy document so you can split it on a sentence by sentence basis.  \n",
    "    with open(name, \"r\", encoding='utf8', errors='ignore') as f:\n",
    "        text = f.read()  # read all lines into a string\n",
    "    doc = nlp(text)\n",
    "    print(\"loaded input document\" + name_elems[5])\n",
    "    # initialise a score per sentence tracker \n",
    "    smell_score_per_sentence = {}\n",
    "    smell_terms_per_sentence = {}\n",
    "    sentence_spans = list(doc.sents)\n",
    "    for (i, item) in enumerate(sentence_spans, start=0):\n",
    "        nSmellWords = 0\n",
    "        for token in item:\n",
    "            isSmellWord = IsSmellWord(token.text)\n",
    "            if \"toevlucht\" in token.text:   # ugly hack but the regexp doesn't seem to work here, very strange \n",
    "                continue\n",
    "            if \"ontvlucht\" in token.text:   # ugly hack but the regexp doesn't seem to work here, very strange \n",
    "                continue\n",
    "            if isSmellWord: \n",
    "                nSmellWords += 1\n",
    "                if i in smell_terms_per_sentence:\n",
    "                    smell_terms_per_sentence[i].append(token.text)\n",
    "                else:\n",
    "                    smell_terms_per_sentence[i] = []\n",
    "                    smell_terms_per_sentence[i].append(token.text)\n",
    "                #print(i, token, item)\n",
    "        smell_score_per_sentence[i] =  nSmellWords\n",
    "    # clean up the dictionary to only consider sentences with smell term matches    \n",
    "    for key, value in smell_score_per_sentence.items():\n",
    "        if value == 0:\n",
    "            del smell_score_per_sentence[key]\n",
    "            break\n",
    "    # this bit should filter out overlapping passages -> it checks if the matching sentences are at least 5 sentences apart \n",
    "    # OK, abandoned because I couldn't get this working \n",
    "    #od = collections.OrderedDict(sorted(smell_score_per_sentence.items()))  \n",
    "    #prev = 0\n",
    "    #smell_score_per_sentence_keys_to_keep = {}\n",
    "    #print(len(od),len(smell_score_per_sentence),len(smell_score_per_sentence_keys_to_keep))\n",
    "    #for x in od:\n",
    "    #    print(x)\n",
    "        \n",
    "    #for k, v in od.items():\n",
    "    #    score = k-prev\n",
    "       # print(k,prev,score)\n",
    "    #    if prev == 0:\n",
    "    #        prev = k\n",
    "    #    if k-prev > 5 and k>5:\n",
    "    #        smell_score_per_sentence_keys_to_keep = {k : v}\n",
    "    #    prev = k\n",
    "    #smell_score_per_sentence = smell_score_per_sentence_keys_to_keep\n",
    "    print(\"done matching\")\n",
    "    # sort the smelliest passage in descending order \n",
    "    a1_sorted_keys = sorted(smell_score_per_sentence, key=smell_score_per_sentence.get, reverse=True)\n",
    "    for r in a1_sorted_keys:\n",
    "        if smell_score_per_sentence[r] > 0:\n",
    "            #print(sentence_spans[r])\n",
    "            for x in range(r-10,r):\n",
    "                try:\n",
    "                    print(sentence_spans[x], file=open(outputfile, \"a\"))\n",
    "                except:\n",
    "                    pass \n",
    "                #print(sentence_spans[x], \"\\n\", file=open(\"output.txt\", \"a\"))\n",
    "            for x in range(r,r+10):\n",
    "                try:\n",
    "                    print(sentence_spans[x], file=open(outputfile, \"a\"))\n",
    "                except:\n",
    "                    pass \n",
    "            print(\"\\n\\n=======\\n\\n\", file=open(outputfile, \"a\"))\n",
    "    for r in a1_sorted_keys:\n",
    "        if smell_score_per_sentence[r] > 0:\n",
    "            print(name_elems[-1], \"\\t\", r, \"\\t\", smell_score_per_sentence[r], \"\\t\", \" \".join(smell_terms_per_sentence[r]), file=open(\"tracker_NL_20_Sept.txt\", \"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust lines 5 and 15 for the right filenames, yeah very sloppy, but I'm invoking the CRAPL License: https://matt.might.net/articles/crapl/ !  \n",
    "import glob\n",
    "\n",
    "counter = 1 \n",
    "for name in glob.glob(\"/Users/marieke/Downloads/OdeuropaBenchmarkTextPrep/oudennieuwoostin01vale*_potentially_smelly_passages.txt\"):\n",
    "    name_elems = name.split(\"/\")\n",
    "    print(name_elems[-1][0:-4])\n",
    "    with open(name, \"r\") as f:\n",
    "        text = f.readlines()\n",
    "    excerpt = \"\"\n",
    "    for x in text:\n",
    "        if \"=======\" in x:\n",
    "        #    print(x)\n",
    "        #\n",
    "            fragmentname = \"NL_TRAVEL_\"+name_elems[-1][0:-4]+\"_\"+str(counter)+'.txt'\n",
    "            counter += 1\n",
    "            print(excerpt, file=open(fragmentname, \"a\"))\n",
    "            excerpt = \"\"\n",
    "        else:\n",
    "            excerpt = excerpt + x \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-graphic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-group",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
